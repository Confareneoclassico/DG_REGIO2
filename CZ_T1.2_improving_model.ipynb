{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seaborn==0.9.0\n",
      "pandas==0.24.2\n",
      "numpy==1.14.5\n",
      "matplotlib==2.2.2\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pkg_resources\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sobol_seq\n",
    "import time\n",
    "import types\n",
    "\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            \n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "            \n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for rq in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pkg_resources\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import sobol_seq\n",
    "import time\n",
    "import types\n",
    "\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            \n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "            \n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Database_Final_UPD(3).xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the series of quasi-random numbers for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quasiRandom_df = pd.DataFrame(sobol_seq.i4_sobol_generate(12,8192))\n",
    "\n",
    "DistributionFiMax = 0.8+quasiRandom_df[0]*0.2\n",
    "DistributionFiMin = 0.2+quasiRandom_df[1]*0.2\n",
    "DistributionFiMaxB = 0.8+quasiRandom_df[6]*0.2\n",
    "DistributionFiMinB = 0.2+quasiRandom_df[7]*0.2\n",
    "\n",
    "Exchange_selector = quasiRandom_df[2].round(0).astype(int)\n",
    "Exchange_selectorB = quasiRandom_df[8].round(0).astype(int)\n",
    "\n",
    "years_residue = (quasiRandom_df[3]*(2016-2007)+1).astype(int)\n",
    "years_residueB = (quasiRandom_df[9]*(2016-2007)+1).astype(int)\n",
    "\n",
    "residual_approach = quasiRandom_df[4].round(0).astype(int)\n",
    "residual_approachB = quasiRandom_df[10].round(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearrange the dataframe for our convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb = df[['ProgrammingPeriod','Country','NUTS1Code','NUTS2Code','Year','CF_TOTAL','EAGGF','ERDF_TOTAL','ESF']].copy()\n",
    "df2 = dfb.melt(id_vars=['ProgrammingPeriod','Country','NUTS1Code','NUTS2Code','Year'],var_name='FundingScheme')\n",
    "df3 = df2.pivot_table(index=['ProgrammingPeriod','FundingScheme','Country','NUTS1Code','NUTS2Code'],columns='Year', values='value')\n",
    "df4 = df3.dropna(how='all').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the slice corresponding to the set we'll be working on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4b = df4.loc['2007-2013','ERDF_TOTAL',:,:,:].droplevel([0,1])\n",
    "\n",
    "df4c = df4.loc['2007-2013','CF_TOTAL',:,:,:].droplevel([0,1])\n",
    "\n",
    "df4d = df4b+df4c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding years before the commencing of the programming periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = []\n",
    "\n",
    "diff = int(np.sort(np.array(list(set(df.ProgrammingPeriod))))[0][5:])+6-\\\n",
    "int(np.sort(np.array(list(set(df.ProgrammingPeriod))))[0][:4])\n",
    "\n",
    "d_Var = pd.Series([k/diff for k in range(1,diff+1)],[y for y in \\\n",
    "range(int(np.sort(np.array(list(set(df.ProgrammingPeriod))))[0][:4]),\n",
    "int(np.sort(np.array(list(set(df.ProgrammingPeriod))))[0][5:])+6)])\n",
    "\n",
    "dummy.append(d_Var)\n",
    "\n",
    "for pp in np.sort(np.array(list(set(df.ProgrammingPeriod))))[1:]:\n",
    "    diff = int(pp[5:])+4-int(pp[:4])\n",
    "    \n",
    "    d_Var = pd.Series([k/diff for k in range(1,diff+1)],[y for y in range(int(pp[:4]),int(pp[5:])+4)])\n",
    "    \n",
    "    dummy.append(d_Var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise the cumulative figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4d.copy()\n",
    "\n",
    "Norm_df6 = ((df5.cumsum(axis=1).T/df5.cumsum(axis=1).max(axis=1).values).T).dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the outcome variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Norm_df6['$\\delta$']=(Norm_df6.iloc[:,-10:]-dummy[-1]).cumsum(axis=1).iloc[:,-1]\n",
    "\n",
    "Norm_df6['$\\mu$']=(Norm_df6['$\\delta$'].max()-Norm_df6['$\\delta$'])/(Norm_df6['$\\delta$'].max()-Norm_df6['$\\delta$'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the trigger for the number of years the residual expenditure gets spread onto on the last eligible year of the programming perido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld1 = []\n",
    "for iq,qr in enumerate(quasiRandom_df[5]):\n",
    "    df8b = Norm_df6.copy()\n",
    "    df8b[0]=(qr*(Norm_df6['$\\mu$']*(len(dummy[-1])-1)).astype(int)).astype(int)+1\n",
    "    for il in range(1,len(dummy[-1])):\n",
    "        df8b[il]=df8b[0]-il\n",
    "    df8b[df8b<1]=1\n",
    "    cd = [il0 for il0 in range(len(dummy[-1]))]\n",
    "    df8b['value']=iq\n",
    "    cd.append('value')\n",
    "    df8b = df8b[cd]\n",
    "    ld1.append(df8b)\n",
    "years = pd.concat(ld1)\n",
    "years.set_index('value', append=True, inplace=True)\n",
    "\n",
    "ld1b = []\n",
    "for iq,qr in enumerate(quasiRandom_df[11]):\n",
    "    df8b = Norm_df6.copy()\n",
    "    df8b[0]=(qr*(Norm_df6['$\\mu$']*(len(dummy[-1])-1)).astype(int)).astype(int)+1\n",
    "    for il in range(1,len(dummy[-1])):\n",
    "        df8b[il]=df8b[0]-il\n",
    "    df8b[df8b<1]=1\n",
    "    cd = [il0 for il0 in range(len(dummy[-1]))]\n",
    "    df8b['value']=iq\n",
    "    cd.append('value')\n",
    "    df8b = df8b[cd]\n",
    "    ld1b.append(df8b)\n",
    "yearsb = pd.concat(ld1b)\n",
    "yearsb.set_index('value', append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the yearly residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A(n):\n",
    "    return [(2**j)/(2**n-1) for j in reversed(range(n))]\n",
    "\n",
    "B9 = []\n",
    "for k in reversed(range(2,11)):\n",
    "    B9.append(pd.DataFrame([A(y) for y in range(1,k)],index=[y for y in range(1,k)],\n",
    "                           columns=[y for y in range(1,k)]).fillna(0).sort_values(by=1,ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C(n):\n",
    "    return [1/n for j in reversed(range(n))]\n",
    "\n",
    "C9 = pd.DataFrame([C(y) for y in range(1,11)],index=[y for y in range(1,11)],\n",
    "                           columns=[y for y in range(1,11)]).fillna(0).sort_values(by=1,ascending=False)\n",
    "\n",
    "D = [C9,pd.DataFrame([A(y) for y in range(1,11)],index=[y for y in range(1,11)],\n",
    "                           columns=[y for y in range(1,11)]).fillna(0).sort_values(by=1,ascending=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Payments = df4b.loc['CZ'].iloc[:,-10:].copy().droplevel(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the member-state database and the set of exchange rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('nuts_prog_kat_Cohesion_codesonly_v3.xlsx',usecols=[0,1,5,6,9,10,11,12])\n",
    "df_REGIO = df1[(df1['CCI'].str.contains(\"161\"))|df1['CCI'].str.contains(\"162\")]\n",
    "df_REGIO_capped = df_REGIO[df_REGIO.year<2017]\n",
    "ER = pd.read_csv('CZK_EURO_historical_exchange_rate.csv')\n",
    "ExchangeRates = ER.copy()\n",
    "ExchangeRates['year']=ER.date.astype(str).str[:4].astype(int)\n",
    "ExchangeRates = ExchangeRates[(ExchangeRates['year']>2006)&(ExchangeRates['year']<2017)]\n",
    "ExchangeRates=ExchangeRates.drop(['date','conf'],axis=1).set_index('year')\n",
    "df_REGIO_yearly = df_REGIO_capped.groupby('year')\n",
    "df_REGIO_su = pd.DataFrame([dfr['EU (czk)'].sum() for idf, dfr in df_REGIO_yearly], \n",
    "                            index=[idf for idf, dfr in df_REGIO_yearly],columns=['sum'])\n",
    "\n",
    "ExchangeRates[1]=ExchangeRates['s1'].mean()\n",
    "exchange_rate = ExchangeRates.T\n",
    "exchange_rate.index = [0,1]\n",
    "\n",
    "\n",
    "df_REGIO_REGIO_yearly=df_REGIO_capped.groupby(['year','NUTS2']).sum()\n",
    "df_REGIO_pv = df_REGIO_REGIO_yearly.pivot_table(index='year', columns='NUTS2', values='EU (czk)').fillna(0).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the distance between the modelled expenditure and the MS incurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expe = pd.concat([Payments.copy() for r in range(len(quasiRandom_df))])\n",
    "\n",
    "Exp = Expe.copy()\n",
    "ExpAB1 = Expe.copy()\n",
    "ExpAB2 = Expe.copy()\n",
    "ExpAB6 = Expe.copy()\n",
    "ExpB = Expe.copy()\n",
    "\n",
    "Exp['r']=np.array([r for r in range(len(quasiRandom_df)) for er in range(len(Payments))])\n",
    "N_df = pd.concat([Norm_df6.loc['CZ','$\\mu$'] for r in range(len(quasiRandom_df))]).droplevel(0)\n",
    "\n",
    "ExpAB1['r']=Exp['r']\n",
    "ExpAB2['r']=Exp['r']\n",
    "ExpAB6['r']=Exp['r']\n",
    "ExpB['r']=Exp['r']\n",
    "\n",
    "Est_expenditure = pd.concat([df_REGIO_pv.copy() for r in range(len(quasiRandom_df))])\n",
    "\n",
    "y_slice = years.loc[pd.IndexSlice['CZ',:,:]].droplevel(0)\n",
    "\n",
    "y_sliceB = yearsb.loc[pd.IndexSlice['CZ',:,:]].droplevel(0)\n",
    "\n",
    "Exp.loc[:,2016]=Expe.loc[:,2016]*(DistributionFiMax[Exp.r].values-N_df*(DistributionFiMax[Exp.r]-\\\n",
    "                                                                        DistributionFiMin[Exp.r]).values)\n",
    "ExpAB1.loc[:,2016]=Expe.loc[:,2016]*(DistributionFiMax[Exp.r].values-N_df*(DistributionFiMax[Exp.r]-\\\n",
    "                                                                        DistributionFiMinB[Exp.r]).values)\n",
    "ExpAB2.loc[:,2016]=Expe.loc[:,2016]*(DistributionFiMaxB[Exp.r].values-N_df*(DistributionFiMaxB[Exp.r]-\\\n",
    "                                                                        DistributionFiMin[Exp.r]).values)\n",
    "ExpAB6.loc[:,2016]=Expe.loc[:,2016]*(DistributionFiMax[Exp.r].values-N_df*(DistributionFiMax[Exp.r]-\\\n",
    "                                                                        DistributionFiMin[Exp.r]).values)\n",
    "ExpB.loc[:,2016]=Expe.loc[:,2016]*(DistributionFiMaxB[Exp.r].values-N_df*(DistributionFiMaxB[Exp.r]-\\\n",
    "                                                                        DistributionFiMinB[Exp.r]).values)\n",
    "\n",
    "D_df = pd.concat([D[row].iloc[years_residue[ir]] for ir, row in residual_approach[Exp.r].iteritems()],axis=1).T.values\n",
    "D_dfAB4 = pd.concat([D[row].iloc[years_residueB[ir]] for ir, row in residual_approach[Exp.r].iteritems()],axis=1).T.values\n",
    "D_dfAB5 = pd.concat([D[row].iloc[years_residue[ir]] for ir, row in residual_approachB[Exp.r].iteritems()],axis=1).T.values\n",
    "D_dfB = pd.concat([D[row].iloc[years_residueB[ir]] for ir, row in residual_approachB[Exp.r].iteritems()],axis=1).T.values\n",
    "\n",
    "D_exchange = pd.concat([exchange_rate.iloc[ir] for ir in Exchange_selector for er in range(len(Payments))],axis=1).T.values\n",
    "D_exchangeB = pd.concat([exchange_rate.iloc[ir] for ir in Exchange_selectorB for er in range(len(Payments))],axis=1).T.values\n",
    "\n",
    "for iy2,y2 in enumerate(reversed(range(2007,Payments.columns.max()))):\n",
    "    Exp[y2] = Expe[y2]*(DistributionFiMax[Exp.r].values-N_df*(DistributionFiMax[Exp.r]-DistributionFiMin[Exp.r]).values)\n",
    "    ExpAB1[y2] = Expe[y2]*(DistributionFiMax[Exp.r].values-N_df*(DistributionFiMax[Exp.r]-DistributionFiMinB[Exp.r]).values)\n",
    "    ExpAB2[y2] = Expe[y2]*(DistributionFiMaxB[Exp.r].values-N_df*(DistributionFiMaxB[Exp.r]-DistributionFiMin[Exp.r]).values)\n",
    "    ExpAB6[y2] = Expe[y2]*(DistributionFiMax[Exp.r].values-N_df*(DistributionFiMax[Exp.r]-DistributionFiMin[Exp.r]).values)\n",
    "    ExpB[y2] = Expe[y2]*(DistributionFiMaxB[Exp.r].values-N_df*(DistributionFiMaxB[Exp.r]-DistributionFiMinB[Exp.r]).values)\n",
    "    \n",
    "    for iy3,y3 in enumerate(reversed(range(y2,Payments.columns.max()))):\n",
    "        Exp[y2]+=Expe[y3+1]*(1-DistributionFiMax[Exp.r].values+N_df*(DistributionFiMax[Exp.r]-\\\n",
    "                DistributionFiMin[Exp.r]).values)*B9[iy3+len(dummy[0])-\\\n",
    "                len(dummy[-1])].loc[y_slice.iloc[:,iy3].values,(y3+1-y2)].values\n",
    "        ExpAB1[y2]+=Expe[y3+1]*(1-DistributionFiMax[Exp.r].values+N_df*(DistributionFiMax[Exp.r]-\\\n",
    "                DistributionFiMinB[Exp.r]).values)*B9[iy3+len(dummy[0])-\\\n",
    "                len(dummy[-1])].loc[y_slice.iloc[:,iy3].values,(y3+1-y2)].values\n",
    "        ExpAB2[y2]+=Expe[y3+1]*(1-DistributionFiMaxB[Exp.r].values+N_df*(DistributionFiMaxB[Exp.r]-\\\n",
    "                DistributionFiMin[Exp.r]).values)*B9[iy3+len(dummy[0])-\\\n",
    "                len(dummy[-1])].loc[y_slice.iloc[:,iy3].values,(y3+1-y2)].values\n",
    "        ExpAB6[y2]+=Expe[y3+1]*(1-DistributionFiMax[Exp.r].values+N_df*(DistributionFiMax[Exp.r]-\\\n",
    "                DistributionFiMin[Exp.r]).values)*B9[iy3+len(dummy[0])-\\\n",
    "                len(dummy[-1])].loc[y_sliceB.iloc[:,iy3].values,(y3+1-y2)].values\n",
    "        ExpB[y2]+=Expe[y3+1]*(1-DistributionFiMax[Exp.r].values+N_df*(DistributionFiMax[Exp.r]-\\\n",
    "                DistributionFiMin[Exp.r]).values)*B9[iy3+len(dummy[0])-\\\n",
    "                len(dummy[-1])].loc[y_sliceB.iloc[:,iy3].values,(y3+1-y2)].values\n",
    "\n",
    "Exp[2007] += Expe[2007]*(1-DistributionFiMax[Exp.r].values+N_df*(DistributionFiMax[Exp.r]-DistributionFiMin[Exp.r]).values)\n",
    "ExpAB1[2007] += Expe[2007]*(1-DistributionFiMax[Exp.r].values+N_df*(DistributionFiMax[Exp.r]-DistributionFiMinB[Exp.r]).values)\n",
    "ExpAB2[2007] += Expe[2007]*(1-DistributionFiMaxB[Exp.r].values+N_df*(DistributionFiMaxB[Exp.r]-DistributionFiMin[Exp.r]).values)\n",
    "ExpAB6[2007] += Expe[2007]*(1-DistributionFiMax[Exp.r].values+N_df*(DistributionFiMax[Exp.r]-DistributionFiMin[Exp.r]).values)\n",
    "ExpB[2007] += Expe[2007]*(1-DistributionFiMaxB[Exp.r].values+N_df*(DistributionFiMaxB[Exp.r]-DistributionFiMinB[Exp.r]).values)\n",
    "\n",
    "Exp = Exp.drop('r',axis=1)\n",
    "ExpAB1 = ExpAB1.drop('r',axis=1)\n",
    "ExpAB2 = ExpAB2.drop('r',axis=1)\n",
    "ExpAB6 = ExpAB6.drop('r',axis=1)\n",
    "ExpB = ExpB.drop('r',axis=1)\n",
    "\n",
    "Excess = (Est_expenditure/D_exchange).sum(axis=1)-pd.concat([Payments.sum(axis=1) for r in range(len(quasiRandom_df))])\n",
    "ExcessB = (Est_expenditure/D_exchangeB).sum(axis=1)-pd.concat([Payments.sum(axis=1) for r in range(len(quasiRandom_df))])\n",
    "\n",
    "Residue = pd.concat([Excess for co in range(10)],axis=1).T.set_index(pd.Index([yRe for yRe in range(2007,2017)])).T\n",
    "ResidueB = pd.concat([ExcessB for co in range(10)],axis=1).T.set_index(pd.Index([yRe for yRe in range(2007,2017)])).T\n",
    "\n",
    "MS_Expenditure = (Est_expenditure/D_exchange-Residue*D_df)\n",
    "MS_ExpenditureAB3 = (Est_expenditure/D_exchangeB-ResidueB*D_df)\n",
    "MS_ExpenditureAB4 = (Est_expenditure/D_exchange-Residue*D_dfAB4)\n",
    "MS_ExpenditureAB5 = (Est_expenditure/D_exchange-Residue*D_dfAB5)\n",
    "MS_ExpenditureB = (Est_expenditure/D_exchangeB-ResidueB*D_dfB)\n",
    "\n",
    "Distance = pd.DataFrame((np.abs(MS_Expenditure-Exp).T/Expe.sum(axis=1)).T.cumsum(axis=1).iloc[:,\n",
    "                -1]).rename(columns={2016:'Distance'})\n",
    "DistanceAB1 = pd.DataFrame((np.abs(MS_Expenditure-ExpAB1).T/Expe.sum(axis=1)).T.cumsum(axis=1).iloc[:,\n",
    "                -1]).rename(columns={2016:'Distance'})\n",
    "DistanceAB2 = pd.DataFrame((np.abs(MS_Expenditure-ExpAB2).T/Expe.sum(axis=1)).T.cumsum(axis=1).iloc[:,\n",
    "                -1]).rename(columns={2016:'Distance'})\n",
    "DistanceAB3 = pd.DataFrame((np.abs(MS_ExpenditureAB3-Exp).T/Expe.sum(axis=1)).T.cumsum(axis=1).iloc[:,\n",
    "                -1]).rename(columns={2016:'Distance'})\n",
    "DistanceAB4 = pd.DataFrame((np.abs(MS_ExpenditureAB4-Exp).T/Expe.sum(axis=1)).T.cumsum(axis=1).iloc[:,\n",
    "                -1]).rename(columns={2016:'Distance'})\n",
    "DistanceAB5 = pd.DataFrame((np.abs(MS_ExpenditureAB5-Exp).T/Expe.sum(axis=1)).T.cumsum(axis=1).iloc[:,\n",
    "                -1]).rename(columns={2016:'Distance'})\n",
    "DistanceAB6 = pd.DataFrame((np.abs(MS_Expenditure-ExpAB6).T/Expe.sum(axis=1)).T.cumsum(axis=1).iloc[:,\n",
    "                -1]).rename(columns={2016:'Distance'})\n",
    "DistanceB = pd.DataFrame((np.abs(MS_ExpenditureB-ExpB).T/Expe.sum(axis=1)).T.cumsum(axis=1).iloc[:,\n",
    "                -1]).rename(columns={2016:'Distance'})\n",
    "\n",
    "Distance['r']=np.array([r for r in range(len(quasiRandom_df)) for er in range(len(Payments))])\n",
    "DistanceAB1['r']=Distance['r']\n",
    "DistanceAB2['r']=Distance['r']\n",
    "DistanceAB3['r']=Distance['r']\n",
    "DistanceAB4['r']=Distance['r']\n",
    "DistanceAB5['r']=Distance['r']\n",
    "DistanceAB6['r']=Distance['r']\n",
    "DistanceB['r']=Distance['r']\n",
    "\n",
    "Distance_df = Distance.pivot_table(values='Distance',index='r',columns='NUTS2')\n",
    "Distance_dfAB1 = DistanceAB1.pivot_table(values='Distance',index='r',columns='NUTS2')\n",
    "Distance_dfAB2 = DistanceAB2.pivot_table(values='Distance',index='r',columns='NUTS2')\n",
    "Distance_dfAB3 = DistanceAB3.pivot_table(values='Distance',index='r',columns='NUTS2')\n",
    "Distance_dfAB4 = DistanceAB4.pivot_table(values='Distance',index='r',columns='NUTS2')\n",
    "Distance_dfAB5 = DistanceAB5.pivot_table(values='Distance',index='r',columns='NUTS2')\n",
    "Distance_dfAB6 = DistanceAB6.pivot_table(values='Distance',index='r',columns='NUTS2')\n",
    "Distance_dfB = DistanceB.pivot_table(values='Distance',index='r',columns='NUTS2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty analysis - distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eg in Distance_df:\n",
    "    sns.distplot(Distance_df[eg],bins=50,vertical=True,color='c')\n",
    "    plt.xlabel('Relative frequency')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.title(eg)\n",
    "    plt.savefig('Uncertainty_analysis_'+str(eg)+'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us now run the sensitivity analysis as to get out the input parameters affect the output uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distance_S = [Distance_df,Distance_dfAB1,Distance_dfAB2,Distance_dfAB3,Distance_dfAB4,Distance_dfAB5,Distance_dfAB6,\n",
    "             Distance_dfB]\n",
    "\n",
    "Distance_dfS= Distance_S[0].sample(frac=1000,replace=True)\n",
    "Distance_dfS.index.name='idx'\n",
    "Distance_S[-1] = Distance_S[-1].reindex(Distance_dfS.index)\n",
    "\n",
    "Indices = []\n",
    "for ds in Distance_S[1:-1]:\n",
    "    Indices.append(Distance_S[-1]*(-Distance_dfS+ds.reindex(Distance_dfS.index)))\n",
    "    Indices.append(0.5*(Distance_dfS-ds.reindex(Distance_dfS.index))**2)\n",
    "    \n",
    "Distance_dfS['r']=np.sort(np.array([r for r in range(len(quasiRandom_df)) for f in range(1000)]))\n",
    "Distance_S[-1]['r']=Distance_dfS['r']\n",
    "Var = pd.concat([Distance_dfS,Distance_S[-1]],sort=True).groupby('r').var().tail()\n",
    "\n",
    "for i in Indices:\n",
    "    i['r']=Distance_dfS['r']\n",
    "    \n",
    "Sensitivity_indices = pd.concat([i.groupby('r').mean()/Var for i in Indices],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'FiMax-model':DistributionFiMax,'FiMin-model':DistributionFiMin,'Exchange_rate':Exchange_selector,\n",
    "              'Excess-years-taken-out-MS':years_residue,'Excess-share-years-taken-out-MS':residual_approach,\n",
    "              'Residual-years-attributed-model':years.loc[pd.IndexSlice['CZ'],:].droplevel(0)[0]}\n",
    "\n",
    "for c in list(set(Sensitivity_indices.columns)):\n",
    "    Sensitivity_indices[c].plot.box()\n",
    "    plt.xticks(range(1,13),list([q+'_'+p for p in parameters.keys() for q in ['S','T']]),rotation='vertical')\n",
    "    plt.title(c)\n",
    "    plt.savefig('Sensitivity_indices_'+str(c)+'.png',bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots for visual inspection of the trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eg in Distance_df:\n",
    "    plt.scatter(years.loc[pd.IndexSlice['CZ'],:].droplevel(0)[0][eg],Distance_df[eg],s=0.01)\n",
    "    plt.xlabel('Residual-years-attributed-model')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.xticks(np.arange(1,years.loc[pd.IndexSlice['CZ'],:].droplevel(0)[0][eg].max()+1))\n",
    "    plt.title(eg)\n",
    "    plt.savefig('Scatter_plot_Residual-years-attributed-model_'+str(eg)+'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us know operate the Kolmorov-Smirnov test in an attempt to perform Monte Carlo filtering as to reduce the distance observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KS_l = []\n",
    "for c in Distance_df:\n",
    "    parameters_l=[]\n",
    "    for p in parameters.keys():\n",
    "        B_dist=parameters[p][Distance_df[c][Distance_df[c]<Distance_df[c].median()].index].sort_values()\n",
    "        B = (B_dist.cumsum()/B_dist.sum()).values\n",
    "        NB_dist=parameters[p][Distance_df[c][Distance_df[c]>Distance_df[c].median()].index].sort_values()\n",
    "        NB = (NB_dist.cumsum()/NB_dist.sum()).values\n",
    "        parameters_l.append(stats.ks_2samp(B,NB)[1])\n",
    "    KS_series = pd.Series(parameters_l,index=parameters.keys())\n",
    "    KS_l.append(KS_series)\n",
    "Kolmorov_Smirnov_df = pd.concat(KS_l,axis=1)\n",
    "Kolmorov_Smirnov_df.columns = Distance_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the KS significant outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kolmorov_Smirnov_df[Kolmorov_Smirnov_df<0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
