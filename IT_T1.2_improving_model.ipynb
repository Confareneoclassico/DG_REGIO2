{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pkg_resources\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import sobol_seq\n",
    "import time\n",
    "import types\n",
    "\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            \n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "            \n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for rq in requirements:\n",
    "    print(\"{}=={}\".format(*rq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pkg_resources\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import sobol_seq\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import types\n",
    "\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            \n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "            \n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for rq in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Database_Final_UPD(3).xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the series of quasi-random numbers for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quasiRandom_df = pd.DataFrame(sobol_seq.i4_sobol_generate(12,8192))\n",
    "\n",
    "DistributionFiMax = 0.8+quasiRandom_df[0]*0.2\n",
    "DistributionFiMin = 0.2+quasiRandom_df[1]*0.2\n",
    "DistributionFiMaxB = 0.8+quasiRandom_df[6]*0.2\n",
    "DistributionFiMinB = 0.2+quasiRandom_df[7]*0.2\n",
    "\n",
    "Residue_selector = quasiRandom_df[2].round(0).astype(int)\n",
    "Residue_selectorB = quasiRandom_df[8].round(0).astype(int)\n",
    "\n",
    "years_residue = (quasiRandom_df[3]*(2016-2007)+1).astype(int)\n",
    "years_residueB = (quasiRandom_df[9]*(2016-2007)+1).astype(int)\n",
    "\n",
    "residual_approach = quasiRandom_df[4].round(0).astype(int)\n",
    "residual_approachB = quasiRandom_df[10].round(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearrange the dataframe for our convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb = df[['ProgrammingPeriod','Country','NUTS1Code','NUTS2Code','Year','CF_TOTAL','EAGGF','ERDF_TOTAL','ESF']].copy()\n",
    "df2 = dfb.melt(id_vars=['ProgrammingPeriod','Country','NUTS1Code','NUTS2Code','Year'],var_name='FundingScheme')\n",
    "df3 = df2.pivot_table(index=['ProgrammingPeriod','FundingScheme','Country','NUTS1Code','NUTS2Code'],columns='Year', values='value')\n",
    "df4 = df3.dropna(how='all').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the slice corresponding to the set we'll be working on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4b = df4.loc[('2007-2013','ERDF_TOTAL')]\n",
    "\n",
    "df4b.loc[pd.IndexSlice[:,:,'ITH1'],:]=df4b.loc[pd.IndexSlice[:,:,'ITH1'],:].values+\\\n",
    "df4b.loc[pd.IndexSlice[:,:,'ITH2'],:].values\n",
    "df4b.loc[pd.IndexSlice[:,:,'ITH1'],:]\n",
    "df4b=df4b.reset_index()\n",
    "df4b.NUTS2Code[df4b.NUTS2Code=='ITH1']='ITH0'\n",
    "df4b = df4b[df4b.NUTS2Code != 'ITH2']\n",
    "df4b=df4b.set_index(['Country','NUTS1Code','NUTS2Code']).sort_index(by=['Country','NUTS1Code','NUTS2Code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding years before the commencing of the programming periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = []\n",
    "\n",
    "diff = int(np.sort(np.array(list(set(df.ProgrammingPeriod))))[0][5:])+6-\\\n",
    "int(np.sort(np.array(list(set(df.ProgrammingPeriod))))[0][:4])\n",
    "\n",
    "d_Var = pd.Series([k/diff for k in range(1,diff+1)],[y for y in \\\n",
    "range(int(np.sort(np.array(list(set(df.ProgrammingPeriod))))[0][:4]),\n",
    "int(np.sort(np.array(list(set(df.ProgrammingPeriod))))[0][5:])+6)])\n",
    "\n",
    "dummy.append(d_Var)\n",
    "\n",
    "for pp in np.sort(np.array(list(set(df.ProgrammingPeriod))))[1:]:\n",
    "    diff = int(pp[5:])+4-int(pp[:4])\n",
    "    \n",
    "    d_Var = pd.Series([k/diff for k in range(1,diff+1)],[y for y in range(int(pp[:4]),int(pp[5:])+4)])\n",
    "    \n",
    "    dummy.append(d_Var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise the cumulative figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4b.copy()\n",
    "\n",
    "Norm_df6 = ((df5.cumsum(axis=1).T/df5.cumsum(axis=1).max(axis=1).values).T).dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the outcome variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Norm_df6['$\\delta$']=(Norm_df6.iloc[:,-10:]-dummy[-1]).cumsum(axis=1).iloc[:,-1]\n",
    "\n",
    "Norm_df6['$\\mu$']=(Norm_df6['$\\delta$'].max()-Norm_df6['$\\delta$'])/(Norm_df6['$\\delta$'].max()-Norm_df6['$\\delta$'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the trigger for the number of years the residual expenditure gets spread onto on the last eligible year of the programming perido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld1 = []\n",
    "for iq,qr in enumerate(quasiRandom_df[5]):\n",
    "    df8b = Norm_df6.copy()\n",
    "    df8b[0]=(qr*(Norm_df6['$\\mu$']*(len(dummy[-1])-1)).astype(int)).astype(int)+1\n",
    "    for il in range(1,len(dummy[-1])):\n",
    "        df8b[il]=df8b[0]-il\n",
    "    df8b[df8b<1]=1\n",
    "    cd = [il0 for il0 in range(len(dummy[-1]))]\n",
    "    df8b['value']=iq\n",
    "    cd.append('value')\n",
    "    df8b = df8b[cd]\n",
    "    ld1.append(df8b)\n",
    "years = pd.concat(ld1)\n",
    "years.set_index('value', append=True, inplace=True)\n",
    "\n",
    "ld1b = []\n",
    "for iq,qr in enumerate(quasiRandom_df[11]):\n",
    "    df8b = Norm_df6.copy()\n",
    "    df8b[0]=(qr*(Norm_df6['$\\mu$']*(len(dummy[-1])-1)).astype(int)).astype(int)+1\n",
    "    for il in range(1,len(dummy[-1])):\n",
    "        df8b[il]=df8b[0]-il\n",
    "    df8b[df8b<1]=1\n",
    "    cd = [il0 for il0 in range(len(dummy[-1]))]\n",
    "    df8b['value']=iq\n",
    "    cd.append('value')\n",
    "    df8b = df8b[cd]\n",
    "    ld1b.append(df8b)\n",
    "yearsb = pd.concat(ld1b)\n",
    "yearsb.set_index('value', append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the yearly residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A(n):\n",
    "    return [2**j/(2**n-1) for j in reversed(range(n))]\n",
    "\n",
    "B9 = []\n",
    "for k in reversed(range(2,11)):\n",
    "    B9.append(pd.DataFrame([A(y) for y in range(1,k)],index=[y for y in range(1,k)],\n",
    "                           columns=[y for y in range(1,k)]).fillna(0).sort_values(by=1,ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And the number of years from which the Excess get taken out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C(n):\n",
    "    return [1/n for j in reversed(range(n))]\n",
    "\n",
    "C9 = pd.DataFrame([C(y) for y in range(1,11)],index=[y for y in range(1,11)],\n",
    "                           columns=[y for y in range(1,11)]).fillna(0).sort_values(by=1,ascending=False)\n",
    "\n",
    "D = [C9,pd.DataFrame([A(y) for y in range(1,11)],index=[y for y in range(1,11)],\n",
    "                           columns=[y for y in range(1,11)]).fillna(0).sort_values(by=1,ascending=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Payments = df4b.loc['IT'].iloc[:,-10:].copy()\n",
    "Payments.index=Payments.index.droplevel(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the member-state figures our data will be compared against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('20181231 Pagamenti ammessi PO 2007-2013.xls',usecols=[0,1,2,3,4,5,7])\n",
    "df_REGIO = df1[(df1['CCI'].str.contains(\"161\"))|df1['CCI'].str.contains(\"162\")]\n",
    "NUTS2_dic = {'ABRUZZO':'ITF1','BASILICATA':'ITF5','CALABRIA':'ITF6','CAMPANIA':'ITF3','EMILIA':'ITH5','FRIULI':'ITH4','LAZIO':'ITI4',\n",
    "'LIGURIA':'ITC3','LOMBARDIA':'ITC4','MARCHE':'ITI3','MOLISE':'ITF2','PIEMONTE':'ITC1','PUGLIA':'ITF4','SARDEGNA':'ITG2','SICILIA':'ITG1',\n",
    "'TOSCANA':'ITI1','TRENTINO':'ITH0','UMBRIA':'ITI2',\"VALLE D'AOSTA\":'ITC2','VENETO':'ITH3'}\n",
    "\n",
    "df_REGIO_capped = df_REGIO[df_REGIO.ANNO<2017]\n",
    "\n",
    "df_REGIO_capped['NUTS2'] = df_REGIO_capped.REGIONE.map(NUTS2_dic)\n",
    "\n",
    "df_REGIO_capped.DESCRIZIONE_PROGRAMMA = df_REGIO_capped.DESCRIZIONE_PROGRAMMA.str.replace(' Romagna', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute the orphan payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REGIO_capped.loc[df_REGIO_capped['NUTS2'].isnull(),'NUTS2'] = \\\n",
    "df_REGIO_capped['DESCRIZIONE_PROGRAMMA'].str.split(expand=True)[3].map(NUTS2_dic)\n",
    "\n",
    "df_REGIO_nonAttributed = df_REGIO_capped[df_REGIO_capped.NUTS2.isna()]\n",
    "df_REGIO_REGIO = df_REGIO_capped.dropna()\n",
    "\n",
    "df_REGIO_REGIO=df_REGIO_REGIO.rename(columns={'ANNO':'Year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REGIO_REGIO_yearly=df_REGIO_REGIO.groupby(['Year','NUTS2']).sum()\n",
    "df_REGIO_REGIO_yearly['Year']=df_REGIO_REGIO_yearly.index.get_level_values(0)\n",
    "df_REGIO_REGIO_yearly=df_REGIO_REGIO_yearly\n",
    "df_REGIO_REGIO_yearly.index= df_REGIO_REGIO_yearly.index.droplevel(0)\n",
    "\n",
    "df_REGIO_pv = df_REGIO_REGIO_yearly.pivot_table(index='Year', columns='NUTS2', values='PAGAMENTO_AMMESSO_UE').fillna(0).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-distribute the residues as per the NUTS2 expenditures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REGIO_multiplier_year=df_REGIO_nonAttributed.groupby('ANNO').sum().T.values*df_REGIO_pv/df_REGIO_pv.sum()\n",
    "\n",
    "df_REGIO_multiplier_year['r']=1\n",
    "\n",
    "df_REGIO_multiplier = df_REGIO_nonAttributed.groupby('ANNO').sum().T.values*\\\n",
    "pd.concat([df_REGIO_pv.sum(axis=1)/df_REGIO_pv.sum(axis=1).sum() for re in range(10)],axis=1)\n",
    "df_REGIO_multiplier['r']=0\n",
    "df_REGIO_multiplier.columns = df_REGIO_multiplier_year.columns\n",
    "\n",
    "df_REGIO_residual = pd.concat([df_REGIO_multiplier,df_REGIO_multiplier_year])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the excess figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Excess = (df_REGIO_multiplier+df_REGIO_pv).sum(axis=1)-Payments.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Residue = pd.concat([Excess for co in range(10)],axis=1).T.set_index(pd.Index([yRe for yRe in range(2007,2017)])).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the distance between the modelled expenditure and the MS incurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expe = pd.concat([Payments.copy() for r in range(len(quasiRandom_df))])\n",
    "\n",
    "Exp = Expe.copy()\n",
    "ExpAB1 = Expe.copy()\n",
    "ExpAB2 = Expe.copy()\n",
    "ExpAB6 = Expe.copy()\n",
    "ExpB = Expe.copy()\n",
    "\n",
    "Exp['r']=np.array([r for r in range(len(quasiRandom_df)) for er in range(len(Payments))])\n",
    "N_df = pd.concat([Norm_df6.loc['IT','$\\mu$'] for r in range(len(quasiRandom_df))])\n",
    "N_df.index = N_df.index.droplevel(0) \n",
    "ExpAB1['r']=Exp['r']\n",
    "ExpAB2['r']=Exp['r']\n",
    "ExpAB6['r']=Exp['r']\n",
    "ExpB['r']=Exp['r']\n",
    "\n",
    "Est_expenditure = pd.concat([df_REGIO_pv.copy() for r in range(len(quasiRandom_df))])\n",
    "Est_residue = pd.concat([Residue.copy() for r in range(len(quasiRandom_df))])\n",
    "\n",
    "y_slice = years.loc[pd.IndexSlice['IT',:,:]]\n",
    "y_slice.index = y_slice.index.droplevel(0)\n",
    "y_sliceB = yearsb.loc[pd.IndexSlice['IT',:,:]]\n",
    "y_sliceB.index = y_sliceB.index.droplevel(0)\n",
    "\n",
    "Exp.loc[:,2016]=Expe.loc[:,2016]*(DistributionFiMax[Exp.r].values-N_df*(DistributionFiMax[Exp.r]-\\\n",
    "                                                                        DistributionFiMin[Exp.r]).values)\n",
    "ExpAB1.loc[:,2016]=Expe.loc[:,2016]*(DistributionFiMax[Exp.r].values-N_df*(DistributionFiMax[Exp.r]-\\\n",
    "                                                                        DistributionFiMinB[Exp.r]).values)\n",
    "ExpAB2.loc[:,2016]=Expe.loc[:,2016]*(DistributionFiMaxB[Exp.r].values-N_df*(DistributionFiMaxB[Exp.r]-\\\n",
    "                                                                        DistributionFiMin[Exp.r]).values)\n",
    "ExpAB6.loc[:,2016]=Expe.loc[:,2016]*(DistributionFiMax[Exp.r].values-N_df*(DistributionFiMax[Exp.r]-\\\n",
    "                                                                        DistributionFiMin[Exp.r]).values)\n",
    "ExpB.loc[:,2016]=Expe.loc[:,2016]*(DistributionFiMaxB[Exp.r].values-N_df*(DistributionFiMaxB[Exp.r]-\\\n",
    "                                                                        DistributionFiMinB[Exp.r]).values)\n",
    "\n",
    "residual = pd.concat(df_REGIO_residual[df_REGIO_residual.r==rs] for rs in Residue_selector)\n",
    "residualB = pd.concat(df_REGIO_residual[df_REGIO_residual.r==rs] for rs in Residue_selectorB)\n",
    "\n",
    "D_df = pd.concat([D[row].iloc[years_residue[ir]] for ir, row in residual_approach[Exp.r].iteritems()],axis=1).T.values\n",
    "D_dfAB4 = pd.concat([D[row].iloc[years_residueB[ir]] for ir, row in residual_approach[Exp.r].iteritems()],axis=1).T.values\n",
    "D_dfAB5 = pd.concat([D[row].iloc[years_residue[ir]] for ir, row in residual_approachB[Exp.r].iteritems()],axis=1).T.values\n",
    "D_dfB = pd.concat([D[row].iloc[years_residueB[ir]] for ir, row in residual_approachB[Exp.r].iteritems()],axis=1).T.values\n",
    "\n",
    "for iy2,y2 in enumerate(reversed(range(2007,Payments.columns.max()))):\n",
    "    Exp[y2] = Expe[y2]*(DistributionFiMax[Exp.r].values-N_df*(DistributionFiMax[Exp.r]-DistributionFiMin[Exp.r]).values)\n",
    "    ExpAB1[y2] = Expe[y2]*(DistributionFiMax[Exp.r].values-N_df*(DistributionFiMax[Exp.r]-DistributionFiMinB[Exp.r]).values)\n",
    "    ExpAB2[y2] = Expe[y2]*(DistributionFiMaxB[Exp.r].values-N_df*(DistributionFiMaxB[Exp.r]-DistributionFiMin[Exp.r]).values)\n",
    "    ExpAB6[y2] = Expe[y2]*(DistributionFiMax[Exp.r].values-N_df*(DistributionFiMax[Exp.r]-DistributionFiMin[Exp.r]).values)\n",
    "    ExpB[y2] = Expe[y2]*(DistributionFiMaxB[Exp.r].values-N_df*(DistributionFiMaxB[Exp.r]-DistributionFiMinB[Exp.r]).values)\n",
    "    \n",
    "    for iy3,y3 in enumerate(reversed(range(y2,Payments.columns.max()))):\n",
    "        Exp[y2]+=Expe[y3+1]*(1-DistributionFiMax[Exp.r].values+N_df*(DistributionFiMax[Exp.r]-\\\n",
    "                DistributionFiMin[Exp.r]).values)*B9[iy3+len(dummy[0])-\\\n",
    "                len(dummy[-1])].loc[y_slice.iloc[:,iy3].values,(y3+1-y2)].values\n",
    "        ExpAB1[y2]+=Expe[y3+1]*(1-DistributionFiMax[Exp.r].values+N_df*(DistributionFiMax[Exp.r]-\\\n",
    "                DistributionFiMinB[Exp.r]).values)*B9[iy3+len(dummy[0])-\\\n",
    "                len(dummy[-1])].loc[y_slice.iloc[:,iy3].values,(y3+1-y2)].values\n",
    "        ExpAB2[y2]+=Expe[y3+1]*(1-DistributionFiMaxB[Exp.r].values+N_df*(DistributionFiMaxB[Exp.r]-\\\n",
    "                DistributionFiMin[Exp.r]).values)*B9[iy3+len(dummy[0])-\\\n",
    "                len(dummy[-1])].loc[y_slice.iloc[:,iy3].values,(y3+1-y2)].values\n",
    "        ExpAB6[y2]+=Expe[y3+1]*(1-DistributionFiMax[Exp.r].values+N_df*(DistributionFiMax[Exp.r]-\\\n",
    "                DistributionFiMin[Exp.r]).values)*B9[iy3+len(dummy[0])-\\\n",
    "                len(dummy[-1])].loc[y_sliceB.iloc[:,iy3].values,(y3+1-y2)].values\n",
    "        ExpB[y2]+=Expe[y3+1]*(1-DistributionFiMax[Exp.r].values+N_df*(DistributionFiMax[Exp.r]-\\\n",
    "                DistributionFiMin[Exp.r]).values)*B9[iy3+len(dummy[0])-\\\n",
    "                len(dummy[-1])].loc[y_sliceB.iloc[:,iy3].values,(y3+1-y2)].values\n",
    "\n",
    "Exp[2007] += Expe[2007]*(1-DistributionFiMax[Exp.r].values+N_df*(DistributionFiMax[Exp.r]-DistributionFiMin[Exp.r]).values)\n",
    "ExpAB1[2007] += Expe[2007]*(1-DistributionFiMax[Exp.r].values+N_df*(DistributionFiMax[Exp.r]-DistributionFiMinB[Exp.r]).values)\n",
    "ExpAB2[2007] += Expe[2007]*(1-DistributionFiMaxB[Exp.r].values+N_df*(DistributionFiMaxB[Exp.r]-DistributionFiMin[Exp.r]).values)\n",
    "ExpAB6[2007] += Expe[2007]*(1-DistributionFiMax[Exp.r].values+N_df*(DistributionFiMax[Exp.r]-DistributionFiMin[Exp.r]).values)\n",
    "ExpB[2007] += Expe[2007]*(1-DistributionFiMaxB[Exp.r].values+N_df*(DistributionFiMaxB[Exp.r]-DistributionFiMinB[Exp.r]).values)\n",
    "\n",
    "Exp = Exp.drop('r',axis=1)\n",
    "ExpAB1 = ExpAB1.drop('r',axis=1)\n",
    "ExpAB2 = ExpAB2.drop('r',axis=1)\n",
    "ExpAB6 = ExpAB6.drop('r',axis=1)\n",
    "ExpB = ExpB.drop('r',axis=1)\n",
    "\n",
    "MS_Expenditure = (Est_expenditure+residual-Est_residue*D_df).drop('r',axis=1)\n",
    "MS_ExpenditureAB3 = (Est_expenditure+residualB-Est_residue*D_df).drop('r',axis=1)\n",
    "MS_ExpenditureAB4 = (Est_expenditure+residual-Est_residue*D_dfAB4).drop('r',axis=1)\n",
    "MS_ExpenditureAB5 = (Est_expenditure+residual-Est_residue*D_dfAB5).drop('r',axis=1)\n",
    "MS_ExpenditureB = (Est_expenditure+residualB-Est_residue*D_dfB).drop('r',axis=1)\n",
    "\n",
    "Distance = pd.DataFrame((np.abs(MS_Expenditure-Exp).T/Expe.sum(axis=1)).T.cumsum(axis=1).iloc[:,\n",
    "                -1]).rename(columns={2016:'Distance'})\n",
    "DistanceAB1 = pd.DataFrame((np.abs(MS_Expenditure-ExpAB1).T/Expe.sum(axis=1)).T.cumsum(axis=1).iloc[:,\n",
    "                -1]).rename(columns={2016:'Distance'})\n",
    "DistanceAB2 = pd.DataFrame((np.abs(MS_Expenditure-ExpAB2).T/Expe.sum(axis=1)).T.cumsum(axis=1).iloc[:,\n",
    "                -1]).rename(columns={2016:'Distance'})\n",
    "DistanceAB3 = pd.DataFrame((np.abs(MS_ExpenditureAB3-Exp).T/Expe.sum(axis=1)).T.cumsum(axis=1).iloc[:,\n",
    "                -1]).rename(columns={2016:'Distance'})\n",
    "DistanceAB4 = pd.DataFrame((np.abs(MS_ExpenditureAB4-Exp).T/Expe.sum(axis=1)).T.cumsum(axis=1).iloc[:,\n",
    "                -1]).rename(columns={2016:'Distance'})\n",
    "DistanceAB5 = pd.DataFrame((np.abs(MS_ExpenditureAB5-Exp).T/Expe.sum(axis=1)).T.cumsum(axis=1).iloc[:,\n",
    "                -1]).rename(columns={2016:'Distance'})\n",
    "DistanceAB6 = pd.DataFrame((np.abs(MS_Expenditure-ExpAB6).T/Expe.sum(axis=1)).T.cumsum(axis=1).iloc[:,\n",
    "                -1]).rename(columns={2016:'Distance'})\n",
    "DistanceB = pd.DataFrame((np.abs(MS_ExpenditureB-ExpB).T/Expe.sum(axis=1)).T.cumsum(axis=1).iloc[:,\n",
    "                -1]).rename(columns={2016:'Distance'})\n",
    "\n",
    "Distance['r']=np.array([r for r in range(len(quasiRandom_df)) for er in range(len(Payments))])\n",
    "DistanceAB1['r']=Distance['r']\n",
    "DistanceAB2['r']=Distance['r']\n",
    "DistanceAB3['r']=Distance['r']\n",
    "DistanceAB4['r']=Distance['r']\n",
    "DistanceAB5['r']=Distance['r']\n",
    "DistanceAB6['r']=Distance['r']\n",
    "DistanceB['r']=Distance['r']\n",
    "\n",
    "Distance_df = Distance.pivot_table(values='Distance',index='r',columns='NUTS2')\n",
    "Distance_dfAB1 = DistanceAB1.pivot_table(values='Distance',index='r',columns='NUTS2')\n",
    "Distance_dfAB2 = DistanceAB2.pivot_table(values='Distance',index='r',columns='NUTS2')\n",
    "Distance_dfAB3 = DistanceAB3.pivot_table(values='Distance',index='r',columns='NUTS2')\n",
    "Distance_dfAB4 = DistanceAB4.pivot_table(values='Distance',index='r',columns='NUTS2')\n",
    "Distance_dfAB5 = DistanceAB5.pivot_table(values='Distance',index='r',columns='NUTS2')\n",
    "Distance_dfAB6 = DistanceAB6.pivot_table(values='Distance',index='r',columns='NUTS2')\n",
    "Distance_dfB = DistanceB.pivot_table(values='Distance',index='r',columns='NUTS2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty analysis - distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eg in Distance_df:\n",
    "    sns.distplot(Distance_df[eg],bins=50,vertical=True,color='c')\n",
    "    plt.xlabel('Relative frequency')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.title(eg)\n",
    "    plt.savefig('Uncertainty_analysis_'+str(eg)+'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us now run the sensitivity analysis as to get out the input parameters affect the output uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distance_S = [Distance_df,Distance_dfAB1,Distance_dfAB2,Distance_dfAB3,Distance_dfAB4,Distance_dfAB5,Distance_dfAB6,\n",
    "             Distance_dfB]\n",
    "\n",
    "Distance_dfS= Distance_S[0].sample(frac=1000,replace=True)\n",
    "Distance_dfS.index.name='idx'\n",
    "Distance_S[-1] = Distance_S[-1].reindex(Distance_dfS.index)\n",
    "\n",
    "Indices = []\n",
    "for ds in Distance_S[1:-1]:\n",
    "    Indices.append(Distance_S[-1]*(-Distance_dfS+ds.reindex(Distance_dfS.index)))\n",
    "    Indices.append(0.5*(Distance_dfS-ds.reindex(Distance_dfS.index))**2)\n",
    "    \n",
    "Distance_dfS['r']=np.sort(np.array([r for r in range(len(quasiRandom_df)) for f in range(1000)]))\n",
    "Distance_S[-1]['r']=Distance_dfS['r']\n",
    "Var = pd.concat([Distance_dfS,Distance_S[-1]],sort=True).groupby('r').var().tail()\n",
    "\n",
    "for i in Indices:\n",
    "    i['r']=Distance_dfS['r']\n",
    "\n",
    "Sensitivity_indices = pd.concat([i.groupby('r').mean()/Var for i in Indices],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'FiMax-model':DistributionFiMax,'FiMin-model':DistributionFiMin,'Non-assigned-MS':Residue_selector,\n",
    "              'Excess-years-taken-out-MS':years_residue,'Excess-share-years-taken-out-MS':residual_approach,\n",
    "              'Residual-years-attributed-model':years.loc[pd.IndexSlice['IT'],:].droplevel(0)[0]}\n",
    "\n",
    "for c in list(set(Sensitivity_indices.columns)):\n",
    "    Sensitivity_indices[c].plot.box()\n",
    "    plt.xticks(range(1,13),list([q+'_'+p for p in parameters.keys() for q in ['S','T']]),rotation='vertical')\n",
    "    plt.title(c)\n",
    "    plt.savefig('Sensitivity_indices_'+str(c)+'.png',bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots for visual inspection of the trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eg in Distance_df:\n",
    "    plt.scatter(years.loc[pd.IndexSlice['IT'],:].droplevel(0)[0][eg],Distance_df[eg],s=0.01)\n",
    "    plt.xlabel('Residual-years-attributed-model')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.xticks(np.arange(1,years.loc[pd.IndexSlice['IT'],:].droplevel(0)[0][eg].max()+1))\n",
    "    plt.title(eg)\n",
    "    plt.savefig(str(eg)+'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us know operate the Kolmorov-Smirnov test in an attempt to perform Monte Carlo filtering as to reduce the distance observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KS_l = []\n",
    "for c in Distance_df:\n",
    "    parameters_l=[]\n",
    "    for p in parameters.keys():\n",
    "        B_dist=parameters[p][Distance_df[c][Distance_df[c]<Distance_df[c].median()].index].sort_values()\n",
    "        B = (B_dist.cumsum()/B_dist.sum()).values\n",
    "        NB_dist=parameters[p][Distance_df[c][Distance_df[c]>Distance_df[c].median()].index].sort_values()\n",
    "        NB = (NB_dist.cumsum()/NB_dist.sum()).values\n",
    "        parameters_l.append(stats.ks_2samp(B,NB)[1])\n",
    "    KS_series = pd.Series(parameters_l,index=parameters.keys())\n",
    "    KS_l.append(KS_series)\n",
    "Kolmorov_Smirnov_df = pd.concat(KS_l,axis=1)\n",
    "Kolmorov_Smirnov_df.columns = Distance_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the KS significant outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kolmorov_Smirnov_df[Kolmorov_Smirnov_df<0.1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
